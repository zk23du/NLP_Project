{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Model, T5Tokenizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6485530187686285\n",
      "Epoch 2, Loss: 0.6472088427593311\n",
      "Epoch 3, Loss: 0.6378699770818154\n",
      "Epoch 4, Loss: 0.648571077734232\n",
      "Epoch 5, Loss: 0.6419491643706957\n",
      "Epoch 6, Loss: 0.6445453949272633\n",
      "Epoch 7, Loss: 0.6477380730211735\n",
      "Epoch 8, Loss: 0.644877896954616\n",
      "Epoch 9, Loss: 0.6402731699248155\n",
      "Epoch 10, Loss: 0.6441772989928722\n",
      "Epoch 1, Loss: 0.6478976121074275\n",
      "Epoch 2, Loss: 0.650036118532482\n",
      "Epoch 3, Loss: 0.6361600835072366\n",
      "Epoch 4, Loss: 0.6475583456064525\n",
      "Epoch 5, Loss: 0.6429201835080197\n",
      "Epoch 6, Loss: 0.6432400157577113\n",
      "Epoch 7, Loss: 0.6493970673335226\n",
      "Epoch 8, Loss: 0.6507728680184013\n",
      "Epoch 9, Loss: 0.649108067939156\n",
      "Epoch 10, Loss: 0.6377576871922142\n"
     ]
    }
   ],
   "source": [
    "# Set device for model training and inference\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_sp.csv'\n",
    "train_data_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_wp.csv'\n",
    "\n",
    "# Initialize DeBERTa tokenizer and model\n",
    "tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "embedding_model = DebertaModel.from_pretrained('microsoft/deberta-base').to(device)\n",
    "embedding_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def make_dataset(data):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    with torch.no_grad():\n",
    "        for idx, row in data.iterrows():\n",
    "            question = row['question']\n",
    "            answer = row['answer']\n",
    "            distractor1 = row['distractor1']\n",
    "            distractor2 = row['distractor2']\n",
    "\n",
    "            # Tokenize and encode each pair\n",
    "            q_a_input = tokenizer(question, answer, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            q_d1_input = tokenizer(question, distractor1, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            q_d2_input = tokenizer(question, distractor2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            \n",
    "            # Obtain pooled output from DeBERTa\n",
    "            q_a_emb = embedding_model(**q_a_input).last_hidden_state[:, 0]\n",
    "            q_d1_emb = embedding_model(**q_d1_input).last_hidden_state[:, 0]\n",
    "            q_d2_emb = embedding_model(**q_d2_input).last_hidden_state[:, 0]\n",
    "\n",
    "            # Concatenate embeddings and append labels\n",
    "            x_train.append(torch.cat([q_a_emb, q_d1_emb], dim=1))\n",
    "            y_train.append(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "            x_train.append(torch.cat([q_a_emb, q_d2_emb], dim=1))\n",
    "            y_train.append(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "            x_train.append(torch.cat([q_a_emb, q_a_emb], dim=1))  # Correct combination for a positive example\n",
    "            y_train.append(torch.tensor([1], dtype=torch.float32))\n",
    "\n",
    "    x_train = torch.stack(x_train).squeeze(1)\n",
    "    y_train = torch.stack(y_train).view(-1, 1)\n",
    "    return x_train, y_train\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(1536, 768)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(768, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def train(model, loader, epochs):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(loader)}')\n",
    "\n",
    "# Example usage\n",
    "# Example usage\n",
    "csv_data_sp = pd.read_csv(train_data_dir_sp)\n",
    "csv_data_wp = pd.read_csv(train_data_dir_wp)\n",
    "x_train_sp, y_train_sp = make_dataset(csv_data_sp)\n",
    "x_train_wp, y_train_wp = make_dataset(csv_data_wp)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader_sp = DataLoader(TensorDataset(x_train_sp, y_train_sp), batch_size=32, shuffle=True)\n",
    "train_loader_wp = DataLoader(TensorDataset(x_train_wp, y_train_wp), batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Model().to(device)\n",
    "train(model, train_loader_sp, 10)\n",
    "train(model, train_loader_wp, 10)\n",
    "torch.save(model, '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/Binary-classification-version/deberta.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def inference(model, question, choices):\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    with torch.no_grad():  # Ensure no gradients are calculated during inference\n",
    "        inputs_question = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs_question = embedding_model(**inputs_question)\n",
    "        question_embedding = outputs_question.last_hidden_state[:, 0]\n",
    "\n",
    "        for choice in choices:\n",
    "            inputs_choice = tokenizer(choice, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs_choice = embedding_model(**inputs_choice)\n",
    "            choice_embedding = outputs_choice.last_hidden_state[:, 0]\n",
    "\n",
    "            # Concatenate along dimension 1 to form a single vector [1536]\n",
    "            combined_input = torch.cat((question_embedding, choice_embedding), dim=1).unsqueeze(0)\n",
    "\n",
    "            logit = model(combined_input)\n",
    "            logits.append(logit)\n",
    "\n",
    "    probabilities = torch.cat(logits).sigmoid()\n",
    "    predicted_index = torch.argmax(probabilities).item()\n",
    "    return choices[predicted_index]\n",
    "\n",
    "\n",
    "\n",
    "def predict(model, df):\n",
    "    for i, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        choices = eval(row['choice_list']) if isinstance(row['choice_list'], str) else row['choice_list']\n",
    "        prediction = inference(model, question, choices)\n",
    "        predicted_id = choices.index(prediction) if prediction in choices else -1\n",
    "        df.loc[i, 'pred_id'] = int(predicted_id)\n",
    "    return df\n",
    "\n",
    "def write_answer_id(df, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for i, row in df.iterrows():\n",
    "            f.write(f\"{int(row['pred_id'])}\\n\")\n",
    "\n",
    "test_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_sp.csv'\n",
    "test_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_wp.csv'\n",
    "test_df_sp = pd.read_csv(test_dir_sp)\n",
    "test_df_wp = pd.read_csv(test_dir_wp)\n",
    "test_df_sp = predict(model, test_df_sp)  # model should be your trained Bert-based model\n",
    "test_df_wp = predict(model, test_df_wp) \n",
    "\n",
    "test_df_sp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_deberta_sp.csv\")\n",
    "test_df_wp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_deberta_wp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_ori: 0.35502958579881655\n",
      "S_sem: 0.34911242603550297\n",
      "S_con: 0.38461538461538464\n",
      "S_ori_sem: 0.3520710059171598\n",
      "S_ori_sem_con: 0.3629191321499014\n",
      "S_overall: 0.3629191321499014\n",
      "W_ori: 0.12878787878787878\n",
      "W_sem: 0.13636363636363635\n",
      "W_con: 0.10606060606060606\n",
      "W_ori_sem: 0.13257575757575757\n",
      "W_ori_sem_con: 0.12373737373737374\n",
      "W_overall: 0.3629191321499014\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame\n",
    "df_sp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_deberta_sp.csv')\n",
    "df_wp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_deberta_wp.csv')\n",
    "\n",
    "# Ensure 'id' is treated as string and replace NaN with an empty string if necessary\n",
    "df_sp['id'] = df_sp['id'].astype(str)\n",
    "df_wp['id'] = df_wp['id'].astype(str)\n",
    "\n",
    "# Define function to calculate accuracy\n",
    "def calculate_accuracy(df, true_label_col, pred_label_col):\n",
    "    correct_predictions = df[df[true_label_col] == df[pred_label_col]]\n",
    "    accuracy = len(correct_predictions) / len(df)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracies for each subset using the updated string column\n",
    "S_ori = calculate_accuracy(df_sp[~df_sp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "S_sem = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "S_con = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "S_ori_sem = calculate_accuracy(df_sp[(df_sp['id'].str.contains(\"_SR\") | ~df_sp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "S_ori_sem_con = calculate_accuracy(df_sp, 'label', 'pred_id')  # For all data\n",
    "S_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "W_ori = calculate_accuracy(df_wp[~df_wp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "W_sem = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "W_con = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "W_ori_sem = calculate_accuracy(df_wp[(df_wp['id'].str.contains(\"_SR\") | ~df_wp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "W_ori_sem_con = calculate_accuracy(df_wp, 'label', 'pred_id')  # For all data\n",
    "W_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'S_ori: {S_ori}')\n",
    "print(f'S_sem: {S_sem}')\n",
    "print(f'S_con: {S_con}')\n",
    "print(f'S_ori_sem: {S_ori_sem}')\n",
    "print(f'S_ori_sem_con: {S_ori_sem_con}')\n",
    "print(f'S_overall: {S_overall}')\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'W_ori: {W_ori}')\n",
    "print(f'W_sem: {W_sem}')\n",
    "print(f'W_con: {W_con}')\n",
    "print(f'W_ori_sem: {W_ori_sem}')\n",
    "print(f'W_ori_sem_con: {W_ori_sem_con}')\n",
    "print(f'W_overall: {W_overall}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
