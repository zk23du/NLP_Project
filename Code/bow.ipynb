{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1768914b44420da8bf775a3670aecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/507 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26358556268c41e2bcb89a8ef3fd5934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/396 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6637175853053728\n",
      "Epoch 2, Loss: 0.6197771237542232\n",
      "Epoch 3, Loss: 0.5613223649561405\n",
      "Epoch 4, Loss: 0.45661520833770436\n",
      "Epoch 5, Loss: 0.3444187470401327\n",
      "Epoch 6, Loss: 0.26402353805800277\n",
      "Epoch 7, Loss: 0.20032437176754078\n",
      "Epoch 8, Loss: 0.15611086746988198\n",
      "Epoch 9, Loss: 0.12726740116098276\n",
      "Epoch 10, Loss: 0.12097894194691132\n",
      "Epoch 1, Loss: 0.86847942753842\n",
      "Epoch 2, Loss: 0.6222567464175978\n",
      "Epoch 3, Loss: 0.568513307132219\n",
      "Epoch 4, Loss: 0.5142681543764315\n",
      "Epoch 5, Loss: 0.4298738680387798\n",
      "Epoch 6, Loss: 0.33290590110578033\n",
      "Epoch 7, Loss: 0.28761267387553263\n",
      "Epoch 8, Loss: 0.23116434169443031\n",
      "Epoch 9, Loss: 0.2175989027478193\n",
      "Epoch 10, Loss: 0.18266800055770496\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Set device for model training and inference\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_sp.csv'\n",
    "train_data_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_wp.csv'\n",
    "# val_data_dir = base_dir + 'val_data.csv'\n",
    "# target_train_data_dir = base_dir + 'pred_train_data.csv'\n",
    "# target_val_data_dir = base_dir + 'pred_val_data.csv'\n",
    "# x_train_tensor_dir = base_dir + 'x_train.pt'\n",
    "# y_train_tensor_dir = base_dir + 'y_train.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load Data\n",
    "csv_data_sp = pd.read_csv(train_data_dir_sp)\n",
    "csv_data_wp = pd.read_csv(train_data_dir_wp)\n",
    "\n",
    "# Build Vocabulary\n",
    "def build_vocabulary(data, max_features=10000):\n",
    "    vectorizer = CountVectorizer(max_features=max_features, stop_words='english')\n",
    "    texts = data['question'].tolist() + data['answer'].tolist() + data['distractor1'].tolist() + data['distractor2'].tolist()\n",
    "    vectorizer.fit(texts)\n",
    "    return vectorizer\n",
    "\n",
    "vectorizer = build_vocabulary(pd.concat([csv_data_sp, csv_data_wp]))\n",
    "\n",
    "# Dataset Preparation\n",
    "def make_dataset(data, vectorizer):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for idx, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        q_vec = vectorizer.transform([row['question']]).toarray()\n",
    "        x_train.append(np.hstack([q_vec, vectorizer.transform([row['answer']]).toarray()]))\n",
    "        y_train.append(1)\n",
    "        x_train.append(np.hstack([q_vec, vectorizer.transform([row['distractor1']]).toarray()]))\n",
    "        y_train.append(0)\n",
    "        x_train.append(np.hstack([q_vec, vectorizer.transform([row['distractor2']]).toarray()]))\n",
    "        y_train.append(0)\n",
    "    return np.array(x_train), np.array(y_train)\n",
    "\n",
    "x_train_sp, y_train_sp = make_dataset(csv_data_sp, vectorizer)\n",
    "x_train_wp, y_train_wp = make_dataset(csv_data_wp, vectorizer)\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "x_train_sp, y_train_sp = torch.FloatTensor(x_train_sp), torch.FloatTensor(y_train_sp).view(-1, 1)\n",
    "x_train_wp, y_train_wp = torch.FloatTensor(x_train_wp), torch.FloatTensor(y_train_wp).view(-1, 1)\n",
    "\n",
    "# DataLoader\n",
    "train_loader_sp = DataLoader(TensorDataset(x_train_sp, y_train_sp), batch_size=32, shuffle=True)\n",
    "train_loader_wp = DataLoader(TensorDataset(x_train_wp, y_train_wp), batch_size=32, shuffle=True)\n",
    "\n",
    "# Model Definition\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        input_size = vectorizer.get_feature_names_out().shape[0] * 2\n",
    "        self.linear1 = nn.Linear(input_size, 768)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(768, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.view(-1, 1)\n",
    "\n",
    "# Initialize and Train Model\n",
    "model = Model().to(device)\n",
    "def train(model, loader, epochs):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(loader)}')\n",
    "\n",
    "train(model, train_loader_sp, 10)\n",
    "train(model, train_loader_wp, 10)\n",
    "torch.save(model, '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/Binary-classification-version/bow.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def inference(model, vectorizer, question, choices):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device  # Get the device type from the model\n",
    "    logits = []\n",
    "\n",
    "    question_vec = vectorizer.transform([question]).toarray()\n",
    "    for choice in choices:\n",
    "        # Create a combined vector for question and choice\n",
    "        combined_vec = np.hstack([question_vec, vectorizer.transform([choice]).toarray()])\n",
    "        combined_tensor = torch.FloatTensor(combined_vec).to(device)\n",
    "\n",
    "        # Forward pass to get logits\n",
    "        logit = model(combined_tensor)\n",
    "        logits.append(logit.item())\n",
    "\n",
    "    # Determine which choice had the highest score\n",
    "    max_index = logits.index(max(logits))\n",
    "    return choices[max_index]\n",
    "\n",
    "def predict(model, df, vectorizer):\n",
    "    predictions = []\n",
    "    for _, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        choices = eval(row['choice_list']) if isinstance(row['choice_list'], str) else row['choice_list']\n",
    "        predicted_choice = inference(model, vectorizer, question, choices)\n",
    "        predicted_id = choices.index(predicted_choice) if predicted_choice in choices else -1\n",
    "        predictions.append(predicted_id)\n",
    "    df['pred_id'] = predictions\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def write_answer_id(df, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for i, row in df.iterrows():\n",
    "            f.write(f\"{int(row['pred_id'])}\\n\")\n",
    "\n",
    "# Example usage\n",
    "test_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_sp.csv'\n",
    "test_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_wp.csv'\n",
    "# Assume model and vectorizer are already initialized\n",
    "test_df_sp = pd.read_csv(test_dir_sp)\n",
    "test_df_wp = pd.read_csv(test_dir_wp)\n",
    "\n",
    "# Predict and handle separately\n",
    "test_df_sp = predict(model, test_df_sp, vectorizer)\n",
    "test_df_wp = predict(model, test_df_wp, vectorizer)\n",
    "\n",
    "\n",
    "test_df_sp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_bow_sp.csv\")\n",
    "test_df_wp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_bow_wp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_ori: 0.9053254437869822\n",
      "S_sem: 0.893491124260355\n",
      "S_con: 0.8816568047337278\n",
      "S_ori_sem: 0.8994082840236687\n",
      "S_ori_sem_con: 0.893491124260355\n",
      "S_overall: 0.893491124260355\n",
      "W_ori: 0.8181818181818182\n",
      "W_sem: 0.7727272727272727\n",
      "W_con: 0.8257575757575758\n",
      "W_ori_sem: 0.7954545454545454\n",
      "W_ori_sem_con: 0.8055555555555556\n",
      "W_overall: 0.893491124260355\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame\n",
    "df_sp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_bow_sp.csv')\n",
    "df_wp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_bow_wp.csv')\n",
    "\n",
    "# Ensure 'id' is treated as string and replace NaN with an empty string if necessary\n",
    "df_sp['id'] = df_sp['id'].astype(str)\n",
    "df_wp['id'] = df_wp['id'].astype(str)\n",
    "\n",
    "# Define function to calculate accuracy\n",
    "def calculate_accuracy(df, true_label_col, pred_label_col):\n",
    "    correct_predictions = df[df[true_label_col] == df[pred_label_col]]\n",
    "    accuracy = len(correct_predictions) / len(df)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracies for each subset using the updated string column\n",
    "S_ori = calculate_accuracy(df_sp[~df_sp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "S_sem = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "S_con = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "S_ori_sem = calculate_accuracy(df_sp[(df_sp['id'].str.contains(\"_SR\") | ~df_sp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "S_ori_sem_con = calculate_accuracy(df_sp, 'label', 'pred_id')  # For all data\n",
    "S_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "W_ori = calculate_accuracy(df_wp[~df_wp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "W_sem = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "W_con = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "W_ori_sem = calculate_accuracy(df_wp[(df_wp['id'].str.contains(\"_SR\") | ~df_wp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "W_ori_sem_con = calculate_accuracy(df_wp, 'label', 'pred_id')  # For all data\n",
    "W_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'S_ori: {S_ori}')\n",
    "print(f'S_sem: {S_sem}')\n",
    "print(f'S_con: {S_con}')\n",
    "print(f'S_ori_sem: {S_ori_sem}')\n",
    "print(f'S_ori_sem_con: {S_ori_sem_con}')\n",
    "print(f'S_overall: {S_overall}')\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'W_ori: {W_ori}')\n",
    "print(f'W_sem: {W_sem}')\n",
    "print(f'W_con: {W_con}')\n",
    "print(f'W_ori_sem: {W_ori_sem}')\n",
    "print(f'W_ori_sem_con: {W_ori_sem_con}')\n",
    "print(f'W_overall: {W_overall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
