{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import T5Model, T5Tokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from transformers import DebertaTokenizer, DebertaModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.679865596195062\n",
      "Epoch 2, Loss: 0.6592123446365198\n",
      "Epoch 3, Loss: 0.6474906044701735\n",
      "Epoch 4, Loss: 0.6443221593896548\n",
      "Epoch 5, Loss: 0.6550957312186559\n",
      "Epoch 6, Loss: 0.6461763220528761\n",
      "Epoch 7, Loss: 0.6457062301536401\n",
      "Epoch 8, Loss: 0.6469847845534483\n",
      "Epoch 9, Loss: 0.6445602333794037\n",
      "Epoch 10, Loss: 0.6483728649715582\n",
      "Epoch 1, Loss: 0.6411899618412319\n",
      "Epoch 2, Loss: 0.635056324695286\n",
      "Epoch 3, Loss: 0.654910219343085\n",
      "Epoch 4, Loss: 0.6439292823013506\n",
      "Epoch 5, Loss: 0.6404409188973276\n",
      "Epoch 6, Loss: 0.6434455592381326\n",
      "Epoch 7, Loss: 0.6390407822634044\n",
      "Epoch 8, Loss: 0.6399764355860258\n",
      "Epoch 9, Loss: 0.6430284694621438\n",
      "Epoch 10, Loss: 0.6459327873430754\n"
     ]
    }
   ],
   "source": [
    "# Set device for model training and inference\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_sp.csv'\n",
    "train_data_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_wp.csv'\n",
    "\n",
    "# Initialize GPT-2 tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to be eos_token\n",
    "embedding_model = GPT2Model.from_pretrained('gpt2').to(device)\n",
    "embedding_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "def make_dataset(data):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    with torch.no_grad():\n",
    "        for idx, row in data.iterrows():\n",
    "            question = row['question']\n",
    "            answer = row['answer']\n",
    "            distractor1 = row['distractor1']\n",
    "            distractor2 = row['distractor2']\n",
    "\n",
    "            # Tokenize and encode each pair\n",
    "            q_a_input = tokenizer(question, answer, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            q_d1_input = tokenizer(question, distractor1, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            q_d2_input = tokenizer(question, distractor2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            # Obtain pooled output from DeBERTa\n",
    "            q_a_emb = embedding_model(**q_a_input).last_hidden_state[:, 0]\n",
    "            q_d1_emb = embedding_model(**q_d1_input).last_hidden_state[:, 0]\n",
    "            q_d2_emb = embedding_model(**q_d2_input).last_hidden_state[:, 0]\n",
    "\n",
    "            # Concatenate embeddings and append labels\n",
    "            x_train.append(torch.cat([q_a_emb, q_d1_emb], dim=1))\n",
    "            y_train.append(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "            x_train.append(torch.cat([q_a_emb, q_d2_emb], dim=1))\n",
    "            y_train.append(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "            x_train.append(torch.cat([q_a_emb, q_a_emb], dim=1))  # Correct combination for a positive example\n",
    "            y_train.append(torch.tensor([1], dtype=torch.float32))\n",
    "\n",
    "    x_train = torch.stack(x_train).squeeze(1)\n",
    "    y_train = torch.stack(y_train).view(-1, 1)\n",
    "    return x_train, y_train\n",
    "\n",
    "def inference(model, question, choices):\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        inputs_question = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        outputs_question = embedding_model(**inputs_question)\n",
    "        question_embedding = outputs_question.last_hidden_state[:, -1]  # Use the last token's embedding\n",
    "\n",
    "        for choice in choices:\n",
    "            inputs_choice = tokenizer(choice, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs_choice = embedding_model(**inputs_choice)\n",
    "            choice_embedding = outputs_choice.last_hidden_state[:, -1]  # Use the last token's embedding\n",
    "\n",
    "            # Concatenate embeddings along the feature dimension\n",
    "            combined_input = torch.cat((question_embedding, choice_embedding), dim=1).unsqueeze(0)  # Create a batch dimension\n",
    "\n",
    "            logit = model(combined_input)\n",
    "            logits.append(logit)\n",
    "\n",
    "    probabilities = torch.cat(logits).sigmoid()\n",
    "    predicted_index = torch.argmax(probabilities).item()\n",
    "    return choices[predicted_index]\n",
    "\n",
    "# Adjust the model to handle input dimension of 2 * embedding size of GPT-2\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        embedding_dim = 768 * 2  # GPT-2 base model has 768 features per token, adjust if using a different model\n",
    "        self.linear1 = nn.Linear(embedding_dim, 768)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(768, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Example usage and training logic remains the same, but ensure all parts are compatible\n",
    "def train(model, loader, epochs):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(loader)}')\n",
    "\n",
    "# Example usage\n",
    "csv_data_sp = pd.read_csv(train_data_dir_sp)\n",
    "csv_data_wp = pd.read_csv(train_data_dir_wp)\n",
    "x_train_sp, y_train_sp = make_dataset(csv_data_sp)\n",
    "x_train_wp, y_train_wp = make_dataset(csv_data_wp)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader_sp = DataLoader(TensorDataset(x_train_sp, y_train_sp), batch_size=32, shuffle=True)\n",
    "train_loader_wp = DataLoader(TensorDataset(x_train_wp, y_train_wp), batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Model().to(device)\n",
    "train(model, train_loader_sp, 10)\n",
    "train(model, train_loader_wp, 10)\n",
    "torch.save(model, '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/Binary-classification-version/gpt.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def predict(model, df):\n",
    "    for i, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        choices = eval(row['choice_list']) if isinstance(row['choice_list'], str) else row['choice_list']\n",
    "        prediction = inference(model, question, choices)\n",
    "        predicted_id = choices.index(prediction) if prediction in choices else -1\n",
    "        df.loc[i, 'pred_id'] = int(predicted_id)\n",
    "    return df\n",
    "\n",
    "def write_answer_id(df, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for i, row in df.iterrows():\n",
    "            f.write(f\"{int(row['pred_id'])}\\n\")\n",
    "\n",
    "test_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_sp.csv'\n",
    "test_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_wp.csv'\n",
    "test_df_sp = pd.read_csv(test_dir_sp)\n",
    "test_df_wp = pd.read_csv(test_dir_wp)\n",
    "test_df_sp = predict(model, test_df_sp)  # model should be your trained Bert-based model\n",
    "test_df_wp = predict(model, test_df_wp) \n",
    "\n",
    "test_df_sp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_gpt_sp.csv\")\n",
    "test_df_wp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_gpt_wp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_ori: 0.1952662721893491\n",
      "S_sem: 0.17159763313609466\n",
      "S_con: 0.25443786982248523\n",
      "S_ori_sem: 0.1834319526627219\n",
      "S_ori_sem_con: 0.20710059171597633\n",
      "S_overall: 0.20710059171597633\n",
      "W_ori: 0.32575757575757575\n",
      "W_sem: 0.3409090909090909\n",
      "W_con: 0.45454545454545453\n",
      "W_ori_sem: 0.3333333333333333\n",
      "W_ori_sem_con: 0.37373737373737376\n",
      "W_overall: 0.20710059171597633\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your DataFrame\n",
    "df_sp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_gpt_sp.csv')\n",
    "df_wp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_gpt_wp.csv')\n",
    "\n",
    "# Ensure 'id' is treated as string and replace NaN with an empty string if necessary\n",
    "df_sp['id'] = df_sp['id'].astype(str)\n",
    "df_wp['id'] = df_wp['id'].astype(str)\n",
    "\n",
    "# Define function to calculate accuracy\n",
    "def calculate_accuracy(df, true_label_col, pred_label_col):\n",
    "    correct_predictions = df[df[true_label_col] == df[pred_label_col]]\n",
    "    accuracy = len(correct_predictions) / len(df)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracies for each subset using the updated string column\n",
    "S_ori = calculate_accuracy(df_sp[~df_sp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "S_sem = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "S_con = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "S_ori_sem = calculate_accuracy(df_sp[(df_sp['id'].str.contains(\"_SR\") | ~df_sp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "S_ori_sem_con = calculate_accuracy(df_sp, 'label', 'pred_id')  # For all data\n",
    "S_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "W_ori = calculate_accuracy(df_wp[~df_wp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "W_sem = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "W_con = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "W_ori_sem = calculate_accuracy(df_wp[(df_wp['id'].str.contains(\"_SR\") | ~df_wp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "W_ori_sem_con = calculate_accuracy(df_wp, 'label', 'pred_id')  # For all data\n",
    "W_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'S_ori: {S_ori}')\n",
    "print(f'S_sem: {S_sem}')\n",
    "print(f'S_con: {S_con}')\n",
    "print(f'S_ori_sem: {S_ori_sem}')\n",
    "print(f'S_ori_sem_con: {S_ori_sem_con}')\n",
    "print(f'S_overall: {S_overall}')\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'W_ori: {W_ori}')\n",
    "print(f'W_sem: {W_sem}')\n",
    "print(f'W_con: {W_con}')\n",
    "print(f'W_ori_sem: {W_ori_sem}')\n",
    "print(f'W_ori_sem_con: {W_ori_sem_con}')\n",
    "print(f'W_overall: {W_overall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
