{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "train_data_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_sp.csv'\n",
    "train_data_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_wp.csv'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "embedding_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "embedding_model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:08<00:00, 60.07it/s]\n",
      "100%|██████████| 396/396 [00:06<00:00, 63.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6522629931569099\n",
      "Epoch 2, Loss: 0.6520435313383738\n",
      "Epoch 3, Loss: 0.6452076447506746\n",
      "Epoch 4, Loss: 0.6461859482030073\n",
      "Epoch 5, Loss: 0.6464430106182893\n",
      "Epoch 6, Loss: 0.6474070772528648\n",
      "Epoch 7, Loss: 0.6456626846144596\n",
      "Epoch 8, Loss: 0.6434557028114796\n",
      "Epoch 9, Loss: 0.6480086954931418\n",
      "Epoch 10, Loss: 0.641064872344335\n",
      "Epoch 1, Loss: 0.6394997207741988\n",
      "Epoch 2, Loss: 0.6431737338241778\n",
      "Epoch 3, Loss: 0.6436468347122795\n",
      "Epoch 4, Loss: 0.6436065309926083\n",
      "Epoch 5, Loss: 0.6357781024355638\n",
      "Epoch 6, Loss: 0.6465994354925657\n",
      "Epoch 7, Loss: 0.6363412533935747\n",
      "Epoch 8, Loss: 0.6389836739552649\n",
      "Epoch 9, Loss: 0.6436994248314908\n",
      "Epoch 10, Loss: 0.637490650540904\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "def make_dataset(data):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "            question = row['question']\n",
    "            answer = row['answer']\n",
    "            distractor1 = row['distractor1']\n",
    "            distractor2 = row['distractor2']\n",
    "\n",
    "            # Tokenize and encode each pair\n",
    "            q_a_input = tokenizer(question, answer, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            q_d1_input = tokenizer(question, distractor1, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            q_d2_input = tokenizer(question, distractor2, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            \n",
    "            # Obtain pooled output from RoBERTa\n",
    "            q_a_emb = embedding_model(**q_a_input).pooler_output\n",
    "            q_d1_emb = embedding_model(**q_d1_input).pooler_output\n",
    "            q_d2_emb = embedding_model(**q_d2_input).pooler_output\n",
    "\n",
    "            # Append embeddings and labels after correct concatenation\n",
    "            x_train.append(torch.cat([q_a_emb, q_a_emb], dim=1))  # Adjust as necessary\n",
    "            y_train.append(torch.tensor([1], dtype=torch.float32))\n",
    "\n",
    "            x_train.append(torch.cat([q_a_emb, q_d1_emb], dim=1))\n",
    "            y_train.append(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "            x_train.append(torch.cat([q_a_emb, q_d2_emb], dim=1))\n",
    "            y_train.append(torch.tensor([0], dtype=torch.float32))\n",
    "\n",
    "    x_train = torch.stack(x_train).squeeze(1)\n",
    "    y_train = torch.stack(y_train).view(-1, 1)\n",
    "    return x_train, y_train\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear1 = nn.Linear(1536, 768)  # First layer to reduce dimension\n",
    "        self.dropout1 = nn.Dropout(0.5)      # Dropout to prevent overfitting\n",
    "        self.relu = nn.ReLU()                # ReLU activation to introduce non-linearity\n",
    "        self.linear2 = nn.Linear(768, 256)   # Further reduce dimension\n",
    "        self.dropout2 = nn.Dropout(0.5)      # Another dropout layer\n",
    "        self.linear3 = nn.Linear(256, 1)     # Final layer to produce output\n",
    "        self.sigmoid = nn.Sigmoid()          # Sigmoid activation to output a probability\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def train(model, loader, epochs):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {total_loss / len(loader)}')\n",
    "\n",
    "def inference(model, question, choices):\n",
    "    model.eval()\n",
    "    logits = []\n",
    "    for choice in choices:\n",
    "        inputs_question = tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        inputs_choice = tokenizer(choice, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        outputs_question = embedding_model(**inputs_question)\n",
    "        outputs_choice = embedding_model(**inputs_choice)\n",
    "\n",
    "        pooled_output_question = outputs_question.pooler_output\n",
    "        pooled_output_choice = outputs_choice.pooler_output\n",
    "        combined_input = torch.cat((pooled_output_question, pooled_output_choice), dim=1)\n",
    "\n",
    "        logit = model(combined_input)\n",
    "        logits.append(logit)\n",
    "\n",
    "    probabilities = torch.cat(logits).sigmoid()\n",
    "    predicted_index = torch.argmax(probabilities).item()\n",
    "    return choices[predicted_index]\n",
    "\n",
    "# Example usage\n",
    "csv_data_sp = pd.read_csv(train_data_dir_sp)\n",
    "csv_data_wp = pd.read_csv(train_data_dir_wp)\n",
    "x_train_sp, y_train_sp = make_dataset(csv_data_sp)\n",
    "x_train_wp, y_train_wp = make_dataset(csv_data_wp)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader_sp = DataLoader(TensorDataset(x_train_sp, y_train_sp), batch_size=32, shuffle=True)\n",
    "train_loader_wp = DataLoader(TensorDataset(x_train_wp, y_train_wp), batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = Model().to(device)\n",
    "train(model, train_loader_sp, 10)\n",
    "train(model, train_loader_wp, 10)\n",
    "torch.save(model, '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/Binary-classification-version/roberta.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "embedding_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "embedding_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "def predict(model, df):\n",
    "    for i, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        # Ensure choices is a list; modify if necessary\n",
    "        choices = eval(row['choice_list']) if isinstance(row['choice_list'], str) else row['choice_list']\n",
    "        prediction = inference(model, question, choices)\n",
    "        predicted_id = choices.index(prediction) if prediction in choices else -1\n",
    "        df.loc[i, 'pred_id'] = int(predicted_id)\n",
    "    return df\n",
    "\n",
    "\n",
    "def write_answer_id(df, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        for i, row in df.iterrows():\n",
    "            f.write(f\"{int(row['pred_id'])}\\n\")\n",
    "\n",
    "\n",
    "test_dir_sp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_sp.csv'\n",
    "test_dir_wp = '/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/train_new_wp.csv'\n",
    "test_df_sp = pd.read_csv(test_dir_sp)\n",
    "test_df_wp = pd.read_csv(test_dir_wp)\n",
    "test_df_sp = predict(model, test_df_sp)  # model should be your trained Bert-based model\n",
    "test_df_wp = predict(model, test_df_wp) \n",
    "\n",
    "test_df_sp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_roberta_sp.csv\")\n",
    "test_df_wp.to_csv(\"/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_roberta_wp.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S_ori: 0.2958579881656805\n",
      "S_sem: 0.28994082840236685\n",
      "S_con: 0.40236686390532544\n",
      "S_ori_sem: 0.29289940828402367\n",
      "S_ori_sem_con: 0.32938856015779094\n",
      "S_overall: 0.32938856015779094\n",
      "W_ori: 0.3787878787878788\n",
      "W_sem: 0.3787878787878788\n",
      "W_con: 0.4090909090909091\n",
      "W_ori_sem: 0.3787878787878788\n",
      "W_ori_sem_con: 0.3888888888888889\n",
      "W_overall: 0.32938856015779094\n"
     ]
    }
   ],
   "source": [
    "# Load your DataFrame\n",
    "df_sp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_roberta_sp.csv')\n",
    "df_wp = pd.read_csv('/home/drishya/drishya/drishya/NLP/SemEval-2024_task9_BRAINTEASER/predictions_roberta_wp.csv')\n",
    "\n",
    "# Ensure 'id' is treated as string and replace NaN with an empty string if necessary\n",
    "df_sp['id'] = df_sp['id'].astype(str)\n",
    "df_wp['id'] = df_wp['id'].astype(str)\n",
    "\n",
    "# Define function to calculate accuracy\n",
    "def calculate_accuracy(df, true_label_col, pred_label_col):\n",
    "    correct_predictions = df[df[true_label_col] == df[pred_label_col]]\n",
    "    accuracy = len(correct_predictions) / len(df)\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracies for each subset using the updated string column\n",
    "S_ori = calculate_accuracy(df_sp[~df_sp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "S_sem = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "S_con = calculate_accuracy(df_sp[df_sp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "S_ori_sem = calculate_accuracy(df_sp[(df_sp['id'].str.contains(\"_SR\") | ~df_sp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "S_ori_sem_con = calculate_accuracy(df_sp, 'label', 'pred_id')  # For all data\n",
    "S_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "W_ori = calculate_accuracy(df_wp[~df_wp['id'].str.contains(\"_\")], 'label', 'pred_id')\n",
    "W_sem = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_SR\")], 'label', 'pred_id')\n",
    "W_con = calculate_accuracy(df_wp[df_wp['id'].str.contains(\"_CR\")], 'label', 'pred_id')\n",
    "W_ori_sem = calculate_accuracy(df_wp[(df_wp['id'].str.contains(\"_SR\") | ~df_wp['id'].str.contains(\"_\"))], 'label', 'pred_id')\n",
    "W_ori_sem_con = calculate_accuracy(df_wp, 'label', 'pred_id')  # For all data\n",
    "W_overall = S_ori_sem_con  # Overall is the same as considering all data\n",
    "\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'S_ori: {S_ori}')\n",
    "print(f'S_sem: {S_sem}')\n",
    "print(f'S_con: {S_con}')\n",
    "print(f'S_ori_sem: {S_ori_sem}')\n",
    "print(f'S_ori_sem_con: {S_ori_sem_con}')\n",
    "print(f'S_overall: {S_overall}')\n",
    "\n",
    "# Print the calculated scores\n",
    "print(f'W_ori: {W_ori}')\n",
    "print(f'W_sem: {W_sem}')\n",
    "print(f'W_con: {W_con}')\n",
    "print(f'W_ori_sem: {W_ori_sem}')\n",
    "print(f'W_ori_sem_con: {W_ori_sem_con}')\n",
    "print(f'W_overall: {W_overall}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None of above.']\n"
     ]
    }
   ],
   "source": [
    "def predict(model, df):\n",
    "    predictions = []\n",
    "    for i, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        choices = row['choice_list']\n",
    "        predicted_choice_index = inference(model, question, choices)\n",
    "        predictions.append(predicted_choice_index)  # Here you append the index, not the text\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Load your evaluation dataset into a DataFrame\n",
    "# Since we can't extract the text from the image, let's assume it's already done.\n",
    "eval_df = pd.DataFrame({\n",
    "    'question': [\"Everyone called him \"\"Batman,\"\" but he knew nothing about bats and thought they were disgusting.He still cherished being referred to as Batman! How is this possible? \"],  # Example question\n",
    "    'choice_list': [[\"He tries to be friendly.\", \"He is afraid others will laugh at him.\", \"He was the star baseball player.\", \"None of above.\"]]  # Example choices\n",
    "})\n",
    "\n",
    "# Predict the answers for the evaluation dataset\n",
    "predictions = predict(model, eval_df)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['None of above.']\n"
     ]
    }
   ],
   "source": [
    "def predict(model, df):\n",
    "    predictions = []\n",
    "    for i, row in df.iterrows():\n",
    "        question = row['question']\n",
    "        choices = row['choice_list']\n",
    "        predicted_choice_index = inference(model, question, choices)\n",
    "        predictions.append(predicted_choice_index)  # Here you append the index, not the text\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Load your evaluation dataset into a DataFrame\n",
    "# Since we can't extract the text from the image, let's assume it's already done.\n",
    "eval_df = pd.DataFrame({\n",
    "    'question': [\"What kind of stock doesn't have shares?\"],  # Example question\n",
    "    'choice_list': [[\"Small-cap stock.\", \"Livestock.\", \"Growth stock.\", \"None of above.\"]]  # Example choices\n",
    "})\n",
    "# Predict the answers for the evaluation dataset\n",
    "predictions = predict(model, eval_df)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
